# -*- coding: utf-8 -*-
"""Working_flan_t5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qKqbaWX3n7Ay5gJfzCySmWAgmgB97yC3
"""

from langchain import HuggingFaceHub
from langchain.embeddings import HuggingFaceHubEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.qa_with_sources import load_qa_with_sources_chain
from langchain.chains import VectorDBQA
from langchain.document_loaders import DirectoryLoader

from langchain.document_loaders import DirectoryLoader

from transformers import T5Tokenizer, T5ForConditionalGeneration

tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-large")
model = T5ForConditionalGeneration.from_pretrained("google/flan-t5-large")

input_text = """
"You're a friendly chatbot to answer questions from the context below.
Answer the question as truthfully as possible using the provided context, and if the answer is not contained within the context text below, say 'I don't know'
 Context:
"""

directory = r'C:\Users\gokul\OneDrive\Desktop\Fourth Square\Interview 2\Docs'  # Update the directory path accordingly

def load_docs(directory):
  loader = DirectoryLoader(directory)
  documents = loader.load()
  return documents

documents = load_docs(directory)
len(documents)

import os
embeddings = HuggingFaceHubEmbeddings()

from langchain.text_splitter import RecursiveCharacterTextSplitter

def split_docs(documents,chunk_size=1000,chunk_overlap=20):
  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
  docs = text_splitter.split_documents(documents)
  return docs

docs = split_docs(documents)
print(len(docs))

from langchain.vectorstores import Chroma
db = Chroma.from_documents(docs, embeddings)

query = "How can I cancel"
num = 3
matching_docs = db.similarity_search(query,num)
print(matching_docs)

for document in matching_docs:
      input_text += document.page_content
print(input_text)

def generate(input_text):
    max_length = tokenizer.model_max_length
    tokens = tokenizer.encode(input_text, truncation=True, max_length=max_length, padding='max_length', return_tensors='pt')
    tokens = tokens
    outputs = model.generate(tokens)
    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(answer)

generate(input_text)

